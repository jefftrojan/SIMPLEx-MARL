{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs/LS6AVstedKX5hElgs2S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jefftrojan/SIMPLEx-MARL/blob/main/simplexMARL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErDexoIh0VnB",
        "outputId": "a9076dfb-e1e1-4065-c04f-9a1b96d92cb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "%pip install gymnasium numpy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Grid environment"
      ],
      "metadata": {
        "id": "a6ecVjWh00aR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class GridWorldEnv(gym.Env):\n",
        "    def __init__(self, grid_size=5, num_agents=2):\n",
        "        super(GridWorldEnv, self).__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.num_agents = num_agents\n",
        "\n",
        "        # Define action and observation spaces\n",
        "        self.action_space = spaces.Discrete(4)  # 4 possible actions: up, down, left, right\n",
        "        self.observation_space = spaces.Box(low=0, high=self.grid_size-1, shape=(num_agents, 2), dtype=np.int32)\n",
        "\n",
        "        # Define rewards and initial agent positions\n",
        "        self.agent_positions = np.zeros((self.num_agents, 2), dtype=np.int32)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the environment and agents to random positions\n",
        "        self.agent_positions = np.random.randint(0, self.grid_size, size=(self.num_agents, 2))\n",
        "        return self.agent_positions\n",
        "\n",
        "    def step(self, actions):\n",
        "        rewards = np.zeros(self.num_agents)\n",
        "        for i, action in enumerate(actions):\n",
        "            if action == 0:  # Move up\n",
        "                self.agent_positions[i][0] = max(0, self.agent_positions[i][0] - 1)\n",
        "            elif action == 1:  # Move down\n",
        "                self.agent_positions[i][0] = min(self.grid_size - 1, self.agent_positions[i][0] + 1)\n",
        "            elif action == 2:  # Move left\n",
        "                self.agent_positions[i][1] = max(0, self.agent_positions[i][1] - 1)\n",
        "            elif action == 3:  # Move right\n",
        "                self.agent_positions[i][1] = min(self.grid_size - 1, self.agent_positions[i][1] + 1)\n",
        "\n",
        "        # Define some reward (e.g., if the agent reaches the bottom-right corner)\n",
        "        for i in range(self.num_agents):\n",
        "            if np.array_equal(self.agent_positions[i], [self.grid_size - 1, self.grid_size - 1]):\n",
        "                rewards[i] = 1  # Give a reward if an agent reaches the bottom-right corner\n",
        "\n",
        "        return self.agent_positions, rewards, False, {}\n",
        "\n",
        "    def render(self):\n",
        "        grid = np.zeros((self.grid_size, self.grid_size), dtype=str)\n",
        "        grid[:] = '.'\n",
        "        for i, pos in enumerate(self.agent_positions):\n",
        "            grid[tuple(pos)] = f'A{i}'\n",
        "        print(\"\\n\".join([\"\".join(row) for row in grid]))\n",
        "\n",
        "# Test the environment\n",
        "env = GridWorldEnv(grid_size=5, num_agents=2)\n",
        "env.reset()\n",
        "env.render()\n",
        "\n",
        "actions = [3, 2]  # Sample actions: Agent 1 moves right, Agent 2 moves left\n",
        "env.step(actions)\n",
        "env.render()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK5AUkDn0gEb",
        "outputId": "5b78f00e-99fe-4057-a0c9-8031ff7c2ebe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".....\n",
            "....A\n",
            ".....\n",
            "...A.\n",
            ".....\n",
            ".....\n",
            "...A.\n",
            ".....\n",
            "....A\n",
            ".....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Q-Learning Agent\n",
        "\n",
        "Since  we have the environment, let’s implement simple Q-learning agents. Each agent will learn independently how to navigate the grid."
      ],
      "metadata": {
        "id": "OCFNd2XE1BEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, env, learning_rate=0.1, discount_factor=0.95, exploration_rate=1.0, exploration_decay=0.99):\n",
        "        self.env = env\n",
        "        self.q_table = np.zeros((env.grid_size, env.grid_size, env.action_space.n))  # Q-values for each state-action pair\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            return self.env.action_space.sample()  # Explore: random action\n",
        "        else:\n",
        "            # Exploit: choose the action with the highest Q-value\n",
        "            x, y = state\n",
        "            return np.argmax(self.q_table[x, y])\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        x, y = state\n",
        "        next_x, next_y = next_state\n",
        "        best_next_action = np.argmax(self.q_table[next_x, next_y])\n",
        "\n",
        "        # Update Q-value using the Bellman equation\n",
        "        self.q_table[x, y, action] = self.q_table[x, y, action] + self.learning_rate * (\n",
        "            reward + self.discount_factor * self.q_table[next_x, next_y, best_next_action] - self.q_table[x, y, action]\n",
        "        )\n",
        "\n",
        "        # Decay exploration rate\n",
        "        self.exploration_rate *= self.exploration_decay\n"
      ],
      "metadata": {
        "id": "WF8E3fII1XZZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Agents in the environment\n",
        "\n",
        "We’ll now run the agents in the environment, have them learn, and print out their progress."
      ],
      "metadata": {
        "id": "f3w0UKyV1mIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create environment and agents\n",
        "env = GridWorldEnv(grid_size=5, num_agents=2)\n",
        "agents = [QLearningAgent(env) for _ in range(env.num_agents)]\n",
        "\n",
        "# Train agents\n",
        "num_episodes = 1000\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    states = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        actions = [agents[i].choose_action(states[i]) for i in range(env.num_agents)]\n",
        "        next_states, rewards, done, _ = env.step(actions)\n",
        "\n",
        "        for i in range(env.num_agents):\n",
        "            agents[i].learn(states[i], actions[i], rewards[i], next_states[i])\n",
        "\n",
        "        states = next_states\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(f\"Episode {episode}\")\n",
        "        env.render()\n"
      ],
      "metadata": {
        "id": "0YtvInpJ1fUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FqBtEs6q14mL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}